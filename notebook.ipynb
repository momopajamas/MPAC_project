{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c626feae-933c-4be9-a3a2-cc38602e357d",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "With the upsurge in Islamophobia rhetoric in recent months that is starkly reminiscient of a post-9/11 climate in the US and the West, the Muslim Public Affairs Committee (MPAC) has tasked the Center for Security, Technology, and Policy (CSTP) with locating trends within this discourse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023be5b-5ee0-4326-a4f6-55f6d150861c",
   "metadata": {},
   "source": [
    " ## Problem Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fd33d-32ae-4494-8b16-07f2cfd37b01",
   "metadata": {},
   "source": [
    "The goal of this project is to identify dominant themes and patterns underpinning Islamophobic rhetoric in order to better inform research and policy recommendations. Specifically, this project focuses on extracting actionable insights by identifying recurring language patterns, narrative frames, and potential changes in discourse that can support effective advocacy, monitoring, and community safety responses. To accomplish this, we analyze a dataset of public posts from X (formerly Twitter) from a variety of users. \n",
    "\n",
    "Because large-scale manual review of social media content is not feasible, this project also includes a lightweight machine learning component designed to support scalable detection. First, we build a baseline NLP classifier that can distinguish between Islamophobic and non-Islamophobic tweets. Then, using the tweets identified as Islamophobic, we apply thematic analysis (e.g., clustering/topic discovery) to surface the most common narratives and rhetorical patterns present in the dataset. This combined approach supports both detection (what content should be flagged for review) and understanding (what narratives are driving the discourse).\n",
    "\n",
    "#### Defining Islamophobia\n",
    "\n",
    "Islamophobia as a concept can encompass a variety of rhetoric, depending on the definition one uses. For the purposes of this project, we will use the definition of Islamophobia used by the [Bridge Initiative at Georgetown University](https://bridge.georgetown.edu/about-us/what-is-islamophobia/), which posits that \"*Islamophobia is an extreme fear of and hostility toward Islam and Muslims which often leads to hate speech, hate crimes, as well as social and political discrimination.*\"\n",
    "\n",
    "We use this definition in order to limit the scope of the investigation specifically to rhetoric that echos fears and hatred of Islam and Muslims specifically. In order to maintain focus, tweets that are critical or antagonistic to Muslim-majority states, for example, would only be considered Islamophobic if the rhetoric used directly or implicitly attacks Islam as a religion or Muslims as a people.\n",
    "\n",
    "### Project Scope\n",
    "- The dataset consists of 1,619 tweets collected from X.\n",
    "- The project creates a small, high-confidence manually labeled subset to train and evaluate a baseline classifier.\n",
    "- The dataset includes a small set of pre-labeled Islamophobic tweets (~90), which are treated as high-confidence positive examples. A small comparison set of non-Islamophobic tweets is created through targeted sampling and lightweight manual review to enable training and evaluation.\n",
    "- The modeling approach prioritizes interpretability and speed, using traditional NLP features (e.g., TF-IDF) and linear models rather than computationally expensive deep learning approaches.\n",
    "- After classification, the subset of Islamophobic tweets is used to perform theme discovery, producing interpretable clusters/topics supported by representative examples and key terms.\n",
    "- Due to the sensitive nature of the domain and the risk of harm from misclassification, the classifier is framed as a decision-support tool rather than an automatic enforcement system.\n",
    "\n",
    "### Success Criteria\n",
    "Success for this project can be measured by answering three questions:\n",
    "\n",
    "**1. Is the classifier functional and reliable enough to support triage?**\n",
    "- Performance will be evaluated using a confusion matrix and metrics such as precision, recall, and F1-score, with a particular emphasis on precision to reduce harmful false positives.\n",
    "\n",
    "**2. Did we learn something real, clear, and repeatable about the discourse?**\n",
    "- Thematic outputs should produce coherent categories of Islamophobic rhetoric (e.g., dehumanization, collective blame, exclusionary policy narratives, conspiracy framing) supported by representative tweets and distinguishing keywords.\n",
    "\n",
    "**4. Can the insights inform real decisions?**\n",
    "- Findings should translate into concrete, stakeholder-relevant outputs such as narrative summaries, “watchlist” language patterns, and recommendations that can inform policy memos, rapid response, or platform monitoring strategies.\n",
    "\n",
    "### Limitations & Ethics\n",
    "Islamophobia detection is highly context-dependent and can involve sarcasm, coded language, and quotation, making the task difficult even for human reviewers. Because the dataset does not include ground-truth labels, this project relies on a small manually labeled subset, which introduces subjectivity and may not reflect all language patterns in the full dataset. The classifier may also produce false positives when tweets mention Islam or Muslims in neutral or advocacy contexts. For these reasons, the model should be treated as a decision-support tool for triage rather than an automatic enforcement mechanism. A production-ready version would require larger labeled data, multiple annotators, and additional bias/fairness evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4def5b5-97f4-4c18-a13b-81765b727851",
   "metadata": {},
   "source": [
    "# II. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f98998-ef5b-483e-af8c-0f43ad6d262b",
   "metadata": {},
   "source": [
    "This dataset contains 1,619 public posts from X (formerly Twitter). Each row represents a single post and includes the post text, metadata about the author account, and engagement metrics. The data is intended to support analysis of Islamophobic discourse by enabling both language-based analysis (what is being said) and impact-based analysis (how widely it spreads).\n",
    "\n",
    "### Features\n",
    "- `Date`: Timestamp associated with the post. This allows trend analysis over time (e.g., spikes in activity or engagement).\n",
    "- `X Accounts`: The username or account identifier that published the post. This can be used to assess repetition, concentration of posting behavior, or high-volume accounts.\n",
    "- `Post Link`: Direct URL to the post for auditing and transparency.\n",
    "- `Post Text`: The main content of the tweet. This is the primary input feature for NLP tasks such as classification and theme discovery.\n",
    "- `Retweets` / `Comments` / `Likes` / `Bookmarks` / `Views`: Engagement metrics that approximate reach and amplification. Views represent the broadest measure of exposure, while retweets and bookmarks may indicate stronger forms of engagement.\n",
    "- `Custom Reports`: While this column is mostly empty, around 90 or so entries are marked as 'Islamophobic.' We will consider these entries as high-confidence labels and use them as a starting point to train our classifier.\n",
    "- `Quick Notes`/ `Legistlative Tracker`/ `Workflow`:  These are almost entirly empty, and the few entries contain nothing of value for our purposes, and we will need to cut these to clean our dataset.\n",
    "\n",
    "This dataset supports three core types of analysis:\n",
    "\n",
    "**1. Classification**: Using the post text to train a classifier that can triage harmful content.\n",
    "\n",
    "**2. Topic Modeling**: Identifying recurring narratives and language patterns within Islamophobic tweets (e.g., stereotypes, collective blame, exclusionary rhetoric).\n",
    "\n",
    "**3. Impact Analysis**: Using engagement metrics, especially views and retweets, to understand which narratives are most amplified and hold higher degrees of salience, and therefore may pose greater public influence or harm.\n",
    "\n",
    "### Quality Considerations\n",
    "\n",
    "Several quality issues are present in this dataset:\n",
    "\n",
    "- Missing or inconsistent labels: The dataset does not appear to have complete, standardized annotation fields. Some rows may contain partial labeling information, while most remain unlabeled.\n",
    "- Context loss: Tweets are short and often depend on context (threads, quotes, sarcasm, or external links or links to other tweets). This creates ambiguity when interpreting intent.\n",
    "- Noise in text: Tweet text may include URLs, mentions (@user), hashtags, emojis, or formatting artifacts that require preprocessing.\n",
    "- Engagement bias: Views and engagement are influenced by account size, virality dynamics, and platform algorithms, not only the content itself, so they should be interpreted as approximate impact signals rather than direct measures of harm.\n",
    "- Sampling bias: The dataset may not represent the full spectrum of Twitter discourse. It may be shaped by how the tweets were collected (search terms, accounts, timeframe).\n",
    "\n",
    "Additionally, the scope of this project and the nature of this dataset combined to create a number of limitations:\n",
    "- Our focus on identifying Islamophobia in particular leads us to exclude rhetoric containing other forms of hate and extremism, such as anti-Hindu or antisemitic rhetoric, which can often emanate from the same source or utilize overlapping forms of rhetoric.\n",
    "- Some tweets are antagonostic towards ethnicities or countries, and do not constitute Islamophobia in the strict sense we outlined above.\n",
    "- A large portion of the Islamophobic tweets present address particular social or political situations, such as controversies over mosques in Texas or the alleged fraud allegations of Somalis in Minnesota, which could potentially limit the extent of generalizability of identified themes.\n",
    "- The vast majority of the tweets deal with the the US context specifically, which could limit the transnational dimensions of Islamophobia.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "To keep the project feasible in a short timeline, the project assumes the following:\n",
    "\n",
    "- The post text contains enough signal to differentiate Islamophobic from non-Islamophobic content at a baseline level.\n",
    "- A small high-confidence labeled subset can provide enough ground truth to evaluate a lightweight baseline model.\n",
    "- The classified tweets can be clustered to extrect recurring themes and trends present within online Islamophobic discourse that can be generalized.\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "For efficiency, we will deploy two models and assess how they perform on training and test data Both models will incorporate **Term Frequency-Inverse Document Frequency (TF-IDF)**, which will be useful in determining the relative significance of terms used across the content under `Post Text` by weighing the frequency of the appearance of those terms within a tweet against their relative rarity across all tweets in the dataset.\n",
    "\n",
    "1. `Logistic Regression`: This will be our baseline model as it is strong in text classification and relatively quick to train. The probabilities given by this model will potentially be useful for threshold tuning, and can provide us with the top words driving predictions.\n",
    "2. `LinearSVM`: This model excels at high-dimensional sparse text, and is likewise quick to train and robust.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "For our purposes of narrative and trend analysis, **both False Positives are more harmful than False Negatives**. This is because, while it would be unfortunate to mistakenly classify tweets as not Islamophobic when they actually are, the impact of this misclassification on our trend analysis would be minimized by the fact that, according to our assumptions, there are trends and recurring themes across these tweets. However, it would be far more detrimental to our analysis if our 'bucket' of Islamophobic tweets were polluted by tweets that are not Islamophobic, as this would distort the themes we extract and the conclusions we draw from them.\n",
    "\n",
    "Therefore, we will be using **Precision** as our primary metric of success, as this evaluates our ability to **keep False Positives out** and generate a high-confidence subset for analysis. In other words, it measures how 'pure' our Islamophobic bucket of tweets is, ensuring that the themes we extract are more cohesive and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5bca7-eb93-4533-a89c-fb25d4766460",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df8caf-6e01-4df3-aeb9-3dc43efa3298",
   "metadata": {},
   "source": [
    "Several steps need to be taken before we are able to begin modeling and analyzing the data.\n",
    "\n",
    "First, since the dataset is unlabeled, we went about labeling a smaller subset of the tweets ourselves by creating a `Label` column and assigning 0 for tweets that are not Islamophobic and 1 for tweets that are Islamophobic. There are 180 tweets total that are labeled with a class balance of 50-50. We will use this subset of tweets for training and testing our classifier models.\n",
    "\n",
    "Next, we will take a number of steps to clean the dataset:\n",
    "1. Removing the following columns as they do not provide information necessary for this investigation: `Quick Notes`, `Custom Reports`, `Workflow`, `Legislative Tracker`.\n",
    "2. Remove data entries with tweets that are null values or only include hyperlinks or photos.\n",
    "3. Replace hyperlinks included in tweets with standard tokens to reduce noise.\n",
    "4. Fixing spelling errors contained in the `Post Text` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ffa45-e60c-4436-87da-69ead2121e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db08a60-6f5c-4c77-8827-8506188e34fb",
   "metadata": {},
   "source": [
    "# III. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8711fda-3d86-4ef2-b24d-b425fab040b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10aa4c35-c532-43d7-b35e-a305fed863f6",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80997554-efcc-4ad2-a389-a2654d808142",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12ea7312-2432-487c-aaff-846fec41b331",
   "metadata": {},
   "source": [
    "# IV. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4b009-30b4-47a7-b7ee-ed8b9981eebc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2436db86-bc61-4858-8cdd-ba21c92dbfd3",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1349df-046a-4c4b-b531-3d6fa2240d94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38d84cd8-fe33-4452-a81b-f7a31cc9927e",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e860e9-c39c-48dc-9a57-a387d5d27b3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c80c11c1-0056-452c-bd70-ecde66b57769",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb772f7-1a7e-4f8f-a0ae-0cb24a71c6b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
